%\documentclass{jss}
\documentclass[nojss]{jss}
\usepackage[OT1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}

%\usepackage{cite}
\usepackage{draftwatermark}
\SetWatermarkText{Very draft}
\SetWatermarkScale{1.0}

%\usepackage{myVignette}

%\VignetteIndexEntry{An introduction to markovchain package}
%\VignetteKeywords{vig1}
%\VignettePackage{lifecontingencies}
% need no \usepackage{Sweave.sty}

%\SweaveOpts{prefix.string=Figures/fig}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual


\author{Giorgio Alfredo Spedicato, PhD CStat ACAS \And 
        Mirko Signorelli, MSc}

\title{The \pkg{markovchain} Package: A Package for Easily Handling Discrete
Markov Chains in \proglang{R}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Giorgio Alfredo Spedicato, Mirko Signorelli} %% comma-separated
\Plaintitle{The markovchaing Package: A Package for Easily Handling Discrete
Markov Chains in R} %% without formatting
\Shorttitle{The markovchain package} %% a short title (if necessary)
%% an abstract and keywords
\Abstract{\pkg{markovchain} aims to fill a gap within R packages providing S4
classes and methods to easily handling discrete markov chains. The S4 class
structure will be presented as well implemented classes and methods. Applied
examples will follow} \Keywords{markov chain, transition probabilities}
\Plainkeywords{markov chain, transition probabilities} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Giorgio Alfredo Spedicato\\
  StatisticalAdvisor\\
  Via Firenze 11
  20037 Italy\\
  Telephone: +39/334/6634384\\
  E-mail: \email{spedygiorgio@gmail.com}\\
  URL: \url{www.statisticaladvisor.com}
}

\Address{Mirko Signorelli\\
\email{signorellimirko@hotmail.it}
}


%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%\SweaveOpts{concordance=TRUE}

<<setup,echo=FALSE, results=hide>>=
	options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
	set.seed(123)
@

\maketitle

\section{Introduction}

Markov chains represent a class of stochastic processes of great interest for the wide spectrum of practical applications. %inserire citazione.
In particular, discrete markov chains permit to model the transition probabilities between possible discrete states by the aid of matrices.
Various R packages deals with Markov chains processes and their applications: \pkg{msm} \citep{msmR} works with Multi-State Models for Panel Data, \pkg{mcmcR} \citep{mcmcR} is only one of the many package that implements Monte Carlo Markov Chain approach for estimating models' parameters, \pkg{hmm} fits hidden markov models taking into account covariates. R statistical environments seems to lack a simple R package that coherently defines S4 classes for discrete Markov chains and that allows the statistical analyst to perform probabilistic analysis and statistical infrence.  \pkg{markovchain} \citep{markovchainR} aims to offer greater flexibility in handling discrete time Markov chains. The paper is structured as it follows: Section~\ref{sec:mathematic} briefly revies mathematic and definitions on discrete Markov chains, Section~\ref{sec:examples} shows applied example of discrete Markov chains in various fields.


\section{Markov chains mathematic revies}\label{sec:mathematic}

\subsection*{Definitions}

A discrete-time Markow chain is a sequence of random variables $X_{1,}X_{2},X_{3},...$
with the property of memorylessness (or Markov property), so that
the next state of $X_{n+1}$ depends on the current state of $X_{n}$
only and doesn't depend from the events that preceded it:
\[
Pr\left(X_{n+1}=x_{n+1}\left|X_{1}=x_{1},X_{2}=x_{2,}...,X_{n}=x_{n}\right.\right)=Pr\left(X_{n+1}=x_{n+1}\left|X_{n}=x_{n}\right.\right).
\]


The set $S=\left\{ s_{1},s_{2},...,s_{r}\right\} $ of possible states
of $X_{j}$ is called state space of the chain. In discrete-time Markov
chain, $S$ is finite or countable. 

A Markow chain is stationary (or time-homogeneous) if $Pr\left(X_{n+1}=x\left|X_{n}=y\right.\right)=Pr\left(X_{n}=x\left|X_{n-1}=y\right.\right)$, in other words, if the underlying transition probabilities do not change as time moves on.

The chain moves successively from one state to another (this change
is called transition or step) and the probability $p_{ij}$ to move
from state $s_{i}$ to state $s_{j}$ is called transition probability:
\[
p_{ij}=Pr\left(X_{1}=s_{j}\left|X_{0}=s_{i}\right.\right).
\]


The probability of going from state $i$ to $j$ in $n$ steps is
$p_{ij}^{(n)}=Pr\left(X_{n}=s_{j}\left|X_{0}=s_{i}\right.\right)$.

If the Markov chain is stationary $p_{ij}=Pr\left(X_{k+1}=s_{j}\left|X_{k}=s_{i}\right.\right)$
and $p_{ij}^{(n)}=Pr\left(X_{n+k}=s_{j}\left|X_{k}=s_{i}\right.\right)$,
where $k>0$.

The probability distributions of transitions from one state to another
can be represented into a transition matrix $P$, in which the element
of position $(i,j)$ is the probability $p_{ij}$; for instance, if
$r=3$ the transition matrix $P$ is
\[
P=\left[\begin{array}{ccc}
p_{11} & p_{12} & p_{13}\\
p_{21} & p_{22} & p_{23}\\
p_{31} & p_{32} & p_{33}
\end{array}\right].
\]


The distribution over the states can be written as a stocastic row
vector $x$: if the current state of $x$ is $s_{2}$, $x=\left(0\,1\,0\right)$.
As a consequence, the relation between $x^{(n+1)}$ and $x^{(n)}$
is $x^{(n+1)}=x^{(n)}P$ and, recursively, $x^{(n+2)}=x^{(n)}P^{2}$,
$x^{(n+k)}=x^{(n)}P^{k},\, k>0$.


\subsection*{Example}

Consider the following numerical example. Suppose we have a Markov
chain with a set of 3 possible states $s_{1}$, $s_{2}$ and $s_{3}$.
Let the transition matrix be

\[
P=\left[\begin{array}{ccc}
0.5 & 0.2 & 0.3\\
0.15 & 0.45 & 0.4\\
0.25 & 0.35 & 0.4
\end{array}\right].
\]


$p_{11}=0.5$ means that the probability that $X_{n+1}=s_{1}$ given
that we observed $X_{n}=s_{1}$ is 0.5, and so on. If in the current
state we have $X_{n}=s_{2}$, then

\[
x^{(n+1)}=\left(0\,1\,0\right)\left[\begin{array}{ccc}
0.5 & 0.2 & 0.3\\
0.15 & 0.45 & 0.4\\
0.25 & 0.35 & 0.4
\end{array}\right]=\left(0.15\,0.45\,0.4\right),
\]


\[
x^{(n+2)}=x^{(n+1)}P=\left(0.15\,0.45\,0.4\right)\left[\begin{array}{ccc}
0.5 & 0.2 & 0.3\\
0.15 & 0.45 & 0.4\\
0.25 & 0.35 & 0.4
\end{array}\right]=\left(0.2425\,0.3725\,0.385\right)
\]


and so on. The last result means that $Pr\left(X_{n+2}=s_{1}\left|X_{n}=s_{2}\right.\right)=0.2425$,
$Pr\left(X_{n+2}=s_{2}\left|X_{n}=s_{2}\right.\right)=0.3725$ and
$Pr\left(X_{n+2}=s_{3}\left|X_{n}=s_{2}\right.\right)=0.385$.


\subsection*{Properties}

A state $s_{j}$ is said to be accessible from a state $s_{i}$ (written
$s_{i}\rightarrow s_{j}$) if a system started in state $s_{i}$ has
a positive probability of transitioning into state $s_{j}$ at a certain
point. If both $s_{i}\rightarrow s_{j}$ and $s_{j}\rightarrow s_{i}$
the states $s_{i}$ and $s_{j}$ are said to communicate.

A state $s_{i}$ has a period $k$ if any return to state $s_{i}$
must occur in multiplies of $k$ steps, that is $k=gcd\left\{ n:Pr\left(X_{n}=s_{i}\left|X_{0}=s_{i}\right.\right)>0\right\} ,$where
'gcd' is the greatest common divisor. If $k=1$ the state is said
to be aperiodic, if $k>1$ the state is periodic with period $k$.

A state $s_{i}$ is said to be transient if, given that we start in
state $s_{i}$, there is a positive probability that we will never
return to $s_{i}$; otherwise, $s_{i}$ is recurrent (or persistent
or absorbing). A Markov chain is absorbing if there is at least one
recurrent state; otherwise, the chain is said to be ergodic (or irreducible)
if it is possible to get to any state from any state.

A Markov chain is said to be regular if some power of the transition
matrix has positive elements only; note that regular chains form a
subset of ergodic chains.

An interesting property of regular Markov chains is that, if $P$
is the $k\times k$ transition matrix and $z=\left(z_{1},...,z_{k}\right)$
is the eigenvector of $P$ having $\sum_{i=1}^{k}z_{i}=1$,
\[
\underset{n\rightarrow\infty}{lim}P^{n}=Z,
\]


where $Z$ is the matrix having all rows equal to $z$.



\section{The structure of the package}\label{sec:structure}

%questo lo faccio io

\subsection{Creating markovchain objects}

The package \pkg{markovchain} contains classes and methods that handle 
markov chain in a convenient manner.\\

The package is loaded within the \proglang{R} command line as follows:

<<load,keep.source=TRUE>>=
library("markovchain")
@



The \code{markovchain} and  \code{markovchainList}  S4 classes \citep(chambers) is defined within the \pkg{markovchain} package as displayed:

<<showClass, echo=FALSE, keep.source=TRUE>>=
showClass("markovchain")
showClass("markovchainList")
@

Any element of \code{markovchain} class is comprised by following slots:
\begin{enumerate}
  \item \code{states}: a character vector, listing the states for which transition probabilities are defined.
  \item \code{byrow}: a logical element, indicating whether transition probabilities are shown by row or by column.
  \item \code{transitionMatrix}: the probabilities of transition matrix.
  \item \code{name}: optional character element to name the Markov chain
\end{enumerate}


\code{markovchain} objects can be created either in a long way, as the following code shows,

<<mcInitLong, keep.source=TRUE>>=
weatherStates<-c("sunny", "cloudy", "rain")
byRow<-TRUE
weatherMatrix<-matrix(data=c(0.70, 0.2,0.1,
                       0.3,0.4, 0.3,
                       0.2,0.45,0.35),byrow=byRow, nrow=3,
                     dimnames=list(weatherStates, weatherStates))
mcWeather<-new("markovchain",states=weatherStates, byrow=byRow, 
               transitionMatrix=weatherMatrix, name="Weather")
@

or in a shorter way, displayed below.

<<mcInitLong, keep.source=TRUE>>=
mcWeather<-new("markovchain", states=c("sunny", "cloudy", "rain"), transitionMatrix=matrix(data=c(0.70, 0.2,0.1,
                       0.3,0.4, 0.3,
                       0.2,0.45,0.35),byrow=byRow, nrow=3), name="Weather")

@

When \code{new("markovchain")} is called alone a defaut Markov chain is created.

<<defaultMc, keep.source=TRUE>>=
defaultMc<-new("markovchain")
@

The quicker form of object creation is made possible thanks to the implemented \code{initialize} S4 method that assures:

\begin{itemize}
  \item the \code{transitionMatrix} to be a transition matrix, i.e., all entries to be probabilities and either all rows or all columns to sum up to one, according to the value of \code{byrow} slot.
  \item the columns and rows nams of \code{transitionMatrix} to be defined and to coincide with \code{states} vector slot. 
\end{itemize}

\code{markovchain} objects can be collected in a list within \code{markovchainList} S4 objects as following example shows.

<<intromcList, keep.source=TRUE>>=

mcList<-new("markovchainList",markovchains=list(mcWeather, defaultMc), name="A list of Markov chains")
@



\subsection{Handling markovchain objects}

\pkg{markovchain} contains two classes, \code{markovchain} and \code{markovchainList}. \code{markovchain} objects handle discrete Markov chains, whilst \code{markovchainList} objects consists in list of \code{markovchain} that can be useful to model non - homogeneous Markov chain processess.\\

Following methods have been implemented within the package for \code{markovchain} and \code{markovchainLists} respectively:

<<showClassesAndMethods, echo=FALSE, keep.source=TRUE>>=
showMethods(class="markovchain")
showMethods(class="markovchainList")
@

Table~\ref{tab:methodsToHandleMc} lists which of implemented methods handle and manipulate \code{markovchain} objects.

\begin{table}[h]
  \centering
  \begin{tabular}{lll}
    \hline
  Method & Purpose \\
    \hline  \hline

  \code{*} & Algebraic operators on the transition matrix.\\
  \code{[} & Direct access to transition matrix elements.\\
  \code{==} & Equality operator on the transition matrix.\\
\code{dim} & Dimenion of the transition matrix.\\
  \code{states} & Defined transition states.\\
  \code{t} & Transposition operator (it switches byrow slot value and modifies the transition matrix coherently).\\
  \code{as} & Operator con switch from \code{markovchain} objects to \code{data.frame} objects and vice - versa.\\
  \hline
\end{tabular}
\caption{\pkg{markovchain} methods: matrix handling.}
\label{tab:methodsToHandleMc}
\end{table}  

Operations on the markovchains objects can be easily performed.
Using the previously defined matrix we can find what is the probability distribution of expected weather states two and  seven days after, given actual state to be cloudy. 

<<operations,keep.source=TRUE>>=
initialState<-c(0,1,0)
after2Days<-initialState*(mcWeather*mcWeather)
after7Days<-initialState*(mcWeather^7)
after2Days
after7Days
@

A similar answer could have been obtained if the probabilities were defined by 
column. A column - defined probability matrix could be set up either creating a new matrix or transposing an existing \code{markovchain} object thanks to the \code{t} vector.

<<operations2,keep.source=TRUE>>=
initialState<-c(0,1,0)
mcWeatherTransposed<-t(mcWeather)
after2Days<-(mcWeatherTransposed*mcWeatherTransposed)*initialState
after7Days<-(mcWeather^7)*initialState
after2Days
after7Days
@

Basing informational methods have been defined for \code{markovchain} objects to quickly get states and dimension.

<<otherMethods, keep.source=TRUE>>=
states(mcWeather)
dim(mcWeather)
@

A direct access to transition probabilities is provided both by \code{transitionProbability} method and "[" method.

<<transProb, keep.source=TRUE>>=
transitionProbability(mcWeather, "cloudy","rain")
mcWeather[2,3]
@

A transition matrix can be displayed using \code{print}, \code{show} methods (the latter being less laconic). Similarly, the underlying transition probability diagram can be plot by the use of \code{plotMc} method that was based on \pkg{igraph} package \citep{pkg:igraph} as Figure~\ref{fig:mcPlot} displays.

<<printAndShow, keep.source=TRUE>>=
print(mcWeather)
show(mcWeather)
@

\begin{figure}
\begin{center}
<<label=mcPlot,fig=TRUE,echo=FALSE>>=
plotMc(mcWeather)
@
\caption{Weather example Markov chain plot}
\label{fig:mcPlot}
\end{center}
\end{figure}

The \pkg{igraph} package \citep{pkg:igraph} is used for plotting. \code{...} additional parameters are passed to \code{graph.adjacency} function to control the graph layout.

Exporting to \code{data.frame} is possible and similarly it is possible to import.

<<exportImport, keep.source=TRUE>>=
mcDf<-as(mcWeather, "data.frame")
mcNew<-as(mcDf, "markovchain")
@

Similarly it is possible to export a \code{markovchain} class toward an adjacency matrix.\\

Non-homogeneous markov chains can be created with the aid of \code{markovchainList} object. The example that follows arises from Health Insurance, where the costs associated to patients in a Continuous Care Health Community (CCHC) are modelled by a non-homogeneous Markov Chain, since the transition probabilities can change by year.

<<cchcMcList, echo=FALSE, keep.source=TRUE>>=
stateNames=c("H","I","D")
Q0<-new("markovchain", states=stateNames, 
        transitionMatrix=matrix(c(0.7, 0.2, 0.1,0.1, 0.6, 0.3,0, 0, 1),byrow=TRUE, nrow=3), name="state t0")
Q1<-new("markovchain", states=stateNames, 
        transitionMatrix=matrix(c(0.5, 0.3, 0.2,0, 0.4, 0.6,0, 0, 1),byrow=TRUE, nrow=3), name="state t1")
Q2<-new("markovchain", states=stateNames, 
        transitionMatrix=matrix(c(0.3, 0.2, 0.5,0, 0.2, 0.8,0, 0, 1),byrow=TRUE,nrow=3), name="state t2")
Q3<-new("markovchain", states=stateNames, transitionMatrix=matrix(c(0, 0, 1,0, 0, 1,0, 0, 1),byrow=TRUE, nrow=3), name="state t3")
mcCCRC<-new("markovchainList",markovchains=list(Q0,Q1,Q2,Q3), name="Continuous Care Health Community")
@

It is possible to perform direct access to \code{markovchainList} elements as well as determining the number of underlying \code{markovchain} objects contained therin in advance.

<<cchcMcList2, keep.source=TRUE>>=
mcCCRC[[1]]
dim(mcCCRC)
@

\subsection{Statistics with markovchain objects}
\subsubsection{Probabilistic analysis}

Table~\ref{tab:methodsToStats} shows methods appliable on \code{markovchain} objects to perform probabilistic analysis. 

\begin{table}[h]
  \centering
  \begin{tabular}{lll}
    \hline
  Method & Purpose \\
    \hline  \hline
  \code{absorbingStates} & it returns the absorbing states of the transition matrix, if any.\\
  \code{steadyStates} & it returns the vector(s) of steady state(s) in matricial form.\\
\hline
\end{tabular}
\caption{\pkg{markovchain} methods: statistical operations.}
\label{tab:methodsToStats}
\end{table}

The steady state(s), also known as stationary distribution(s),  of the Markov chains are identified by the following algorithm:
\begin{enumerate}
  \item decompose the Markov Chain in eigenvalues and eigenvectors.
  \item consider only eigenvectors corresponding to eigenvalues equal to one.
  \item normalize such eigenvalues so the sum of their components to total one.
\end{enumerate}

The result is returned in matricial form.

<<steadyStates, keep.source=TRUE>>=
steadyStates(mcWeather)
@

It is possible a Markov chain to have more than one stationary distribuition, as the gambler ruin example shows.

<<gamblerRuin, keep.source=TRUE>>=
gamblerRuinMarkovChain<-function(moneyMax, prob=0.5) {
  require(matlab)
  matr<-zeros(moneyMax+1)
  states<-as.character(seq(from=0, to=moneyMax, by=1))
  rownames(matr)=states; colnames(matr)=states
  matr[1,1]=1;matr[moneyMax+1,moneyMax+1]=1
  for(i in 2:moneyMax)
  {
    matr[i,i-1]=1-prob;matr[i,i+1]=prob
  }
  out<-new("markovchain",  
           transitionMatrix=matr, 
           name=paste("Gambler ruin",moneyMax,"dim",sep=" ")
           )
  return(out)
}

mcGR4<-gamblerRuinMarkovChain(moneyMax=4, prob=0.5)
steadyStates(mcGR4)
@

Any absorbing state is determined by the inspection of results returned by \code{steadyStates} method.

<<absorbingStates, keep.source=TRUE>>=
absorbingStates(mcGR4)
absorbingStates(mcWeather)
@



\subsubsection{Statistical analysis}

Table~\ref{tab:packageFuns} lists functions (and their purpose) as implemented within the package that helps to fit and simulate discrete time Markov chains.

\begin{table}[h]
  \centering
  \begin{tabular}{lll}
    \hline
  Function & Purpose \\
    \hline  \hline
  \code{markovchainFit} & function to return fitten markov chain for a given sequence.\\
   \code{rmarkovchain} & function to sample from \code{markovchain} or \code{markovchainList} objects.\\
    \hline
\end{tabular}
\caption{\pkg{markovchain} statistical functions.}
\label{tab:packageFuns}
\end{table}  


Simulating a random sequence from an underlying Markov chain is quite easy thanks to the function \code{rmarkovchain}.
The following code generates a "year" of weather states according to \cite{mcWeather} underlying markovian stochastic process.

<<simulatingAMarkovChain, keep.source=TRUE>>=
weathersOfDays<-rmarkovchain(n=365,object=mcWeather,t0="sunny")
weathersOfDays[1:30]
@

Similarly, it is possible to simulate one o more sequence from a non-homogeneous markov chain, as the following code (applied on CCHC example) displays.

<<simulatingAListOfMarkovChain, keep.source=TRUE>>=
patientStates<-rmarkovchain(n=5, object=mcCCRC,t0="H",include.t0=TRUE)
patientStates[1:10,]
@

Similarly, a \code{markovchain} object can be fit from given data.
The most straightforward approach is maximum likelihood.

<<fitMcbyMLE, keep.source=TRUE>>=
weatherFittedMLE<-markovchainFit(data=weathersOfDays, method="mle")
weatherFittedMLE$estimate
@

Nevertheless a bootstrap version of maximum likehihood has been developed in order to assess the variability of estimate.

<<fitMcbyBootStrap, keep.source=TRUE>>=
weatherFittedBOOT<-markovchainFit(data=weathersOfDays, method="bootstrap",nboot=50)
weatherFittedBOOT$estimate
weatherFittedBOOT$standardError
@

%qui presenteremmo le tuo funzioni Matlab -> R dopo che le hai doverosamente sistemate


\section{Applied examples}\label{sec:examples}

%Qui lo facciamo io e te

\subsection{Actuarial examples}

Markov chains are widely applied in the fields of actuarial science. Actuaries quantify the risk inherent in insurance contracts evaluating the premium of insurance contract to be sold (therefore covering future risk) and evaluating the actuarial reseves of existing portfolios (the liabilities in terms of benefits or claims payments due to policyholder arising from previously sold contracts).\\
Key quantities of actuarial interest are: the expected present value of future benefits, $PVFB$, the (periodic) benefit premium, $P$, and the present value of future premium $PVFP$. A level benefit premium could be set equating at the beginning of the contract $PVFB=PVFP$. After the beginning of the contract the benefit reserve is the differenbe between $PVFB$ and $PVFP$.
The first example shows the pricing and reserving of a (simple) health insurance contract. The second example analyze the evolution of a MTPL portfolio characterized by Bonus Malus experience rating feature.

\subsubsection{Health insurance example}

The example comes from \cite{deshmukh2012multiple}. The interest rate is 5\%, benefits are payable upon death (1000) and disability (500). Premiums are payable at the beginning of period only if policyholder is active. The contract term is three years 

<<healthIns1, keep.source=TRUE>>=

mcHI=new("markovchain", states=c("active", "disable", "withdrawn", "death"),
         transitionMatrix=matrix(c(0.5,.25,.15,.1,
                                   0.4,0.4,0.0,.2,
                                   0,0,1,0,
                                   0,0,0,1), byrow=TRUE, nrow=4))
         

benefitVector=as.matrix(c(0,0,500,1000))

@

The policyholders is active at $T_0$. Therefore the expected states at $T_1, \ldots T_3$ are calculated as shown.

<<healthIns2, keep.source=TRUE>>=
T0=t(as.matrix(c(1,0,0,0)))
T1=T0*mcHI
T2=T1*mcHI
T3=T2*mcHI
@

Therefore the present value of future benefit at T0 is

<<healthIns3, keep.source=TRUE>>=
PVFB=T0%*%benefitVector*1.05^-0+T1%*%benefitVector*1.05^-1+T2%*%benefitVector*1.05^-2+T3%*%benefitVector*1.05^-3
@

and the yearly premium payable whether the insured is alive is 

<<healthIns4, keep.source=TRUE>>=
P=PVFB/(T0[1]*1.05^-0+T1[1]*1.05^-1+T2[1]*1.05^-2)
@

The reserve at the beginning of year two, in case of the insured being alive, is

<<healthIns5, keep.source=TRUE>>=
PVFB=(T2%*%benefitVector*1.05^-1+T3%*%benefitVector*1.05^-2)
PVFP=P*(T1[1]*1.05^-0+T2[1]*1.05^-1)
V=PVFB-PVFP
V
@


\section{Aknowledgments}\label{sec:aknowledgements}



\bibliography{markovchainBiblio}



\end{document}
